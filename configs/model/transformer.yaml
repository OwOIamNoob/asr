_target_: src.models.transformer_module.TransformerLitModule

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.001

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

encoder:
  _target_: src.models.transformer.model.encoder.Encoder  
  vocab_size: 10000
  input_dim: 300
  d_ff: 512
  d_model: 256
  num_heads: 4
  num_layers: 4
  dropout_p: 0.1
  pad_id: 0
  use_embedding: False

decoder:
  _target_: src.models.transformer.model.decoder.Decoder
  vocab_size: 10000
  input_dim: 300
  d_ff: 512
  d_model: 256
  num_heads: 4
  num_layers: 4
  dropout_p: 0.1
  max_length: 256
  pad_id: 0
  sos_id: 1
  eos_id: 2
  use_embedding: False

# compile model for faster training with pytorch 2.0
compile: false

pad_id: 0
sos_id: 1
eos_id: 2
max_length: 256

teacher_forcing_ratio: 0.6
use_embedding: False
